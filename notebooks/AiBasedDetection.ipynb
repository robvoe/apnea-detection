{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Dict, Iterable\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from ai_based.data_handling.ai_datasets import AiDataset\n",
    "from ai_based.data_handling.training_batch import TrainingBatch\n",
    "from ai_based.networks import MLP, Cnn1D\n",
    "from ai_based.utilities.evaluators import ConfusionMatrixEvaluator\n",
    "from util.paths import RESULTS_PATH_AI, TRAIN_TEST_SPLIT_YAML, DATA_PATH\n",
    "from util.train_test_split import read_train_test_split_yaml\n",
    "from ai_based.utilities.evaluators import BaseEvaluator\n",
    "from util.datasets import GroundTruthClass, RespiratoryEvent, RespiratoryEventType, RESPIRATORY_EVENT_TYPE__GROUND_TRUTH_CLASS\n",
    "from util.mathutil import cluster_1d, IntRange\n",
    "from ai_based.utilities.inference import retrieve_respiratory_events\n",
    "\n",
    "# Some preparations to pretty-print tensors & ndarrays\n",
    "np.set_printoptions(edgeitems=10)\n",
    "np.core.arrayprint._line_width = 400\n",
    "torch.set_printoptions(linewidth=400)\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "n_cpu_cores = len(os.sched_getaffinity(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model, training config, training logs, etc.\n",
    "- Training runs are organized as so-called **experiments**.\n",
    "- An experiment may be run multiple times in different model & hyper-parameter configurations. Each run is called a **combination**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EXPERIMENT_DIR = RESULTS_PATH_AI / \"cnn-3-gt_point-bs128\"\n",
    "COMBINATION_DIR = EXPERIMENT_DIR / \"combination_0\"\n",
    "REPETITION_DIR = COMBINATION_DIR / \"repetition_0\"\n",
    "\n",
    "assert RESULTS_PATH_AI.is_dir() and RESULTS_PATH_AI.exists()\n",
    "assert EXPERIMENT_DIR.is_dir() and EXPERIMENT_DIR.exists()\n",
    "assert COMBINATION_DIR.is_dir() and COMBINATION_DIR.exists()\n",
    "assert REPETITION_DIR.is_dir() and REPETITION_DIR.exists()\n",
    "\n",
    "# log = torch.load(REPETITION_DIR / \"log.pt\", map_location=torch.device(\"cpu\"))\n",
    "# final_validation_eval_results = torch.load(REPETITION_DIR / \"eval.pt\", map_location=torch.device(\"cpu\"))\n",
    "config = torch.load(EXPERIMENT_DIR / \"config.pt\", map_location=torch.device(\"cpu\"))\n",
    "hyperparams = torch.load(COMBINATION_DIR / \"params.pt\", map_location=torch.device(\"cpu\"))\n",
    "if (REPETITION_DIR / \"weights.pt\").exists():\n",
    "    weights = torch.load(REPETITION_DIR / \"weights.pt\", map_location=torch.device(\"cpu\"))\n",
    "elif (REPETITION_DIR / \"checkpoint_best_weights.pt\").exists():\n",
    "    weights = torch.load(REPETITION_DIR / \"checkpoint_best_weights.pt\", map_location=torch.device(\"cpu\"))\n",
    "else:\n",
    "    raise RuntimeError(\"No weights file found\")\n",
    "\n",
    "model = hyperparams[\"model\"](hyperparams[\"model_config\"])\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(log[\"training\"][\"loss\"]))\n",
    "log[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot parameters that were logged during training\n",
    "These are, amongst others:\n",
    "- Training loss\n",
    "- Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(24, 36))\n",
    "\n",
    "axes[0, 0].set_title(\"Loss\")\n",
    "axes[0, 0].plot(log[\"training\"][\"loss\"])\n",
    "\n",
    "axes[0, 1].set_title(\"Weighted mean IoU\")\n",
    "# axes[0, 1].plot(training_miou, label=\"training\")\n",
    "# axes[0, 1].plot(validation_miou, label=\"validation\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].set_title(\"Detection Recall\")\n",
    "# axes[1, 0].plot(log[\"training\"][\"detection_recall\"], label=\"training\")\n",
    "# axes[1, 0].plot(log[\"validation\"][\"detection_recall\"], label=\"validation\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].set_title(\"Placement Accuracy\")\n",
    "# axes[1, 1].plot(log[\"training\"][\"placement_accuracy\"], label=\"training\")\n",
    "# axes[1, 1].plot(log[\"validation\"][\"placement_accuracy\"], label=\"validation\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "axes[2, 0].set_title(\"Object Height Accuracy\")\n",
    "# axes[2, 0].plot(log[\"training\"][\"height_classification_accuracy\"], label=\"training\")\n",
    "# axes[2, 0].plot(log[\"validation\"][\"height_classification_accuracy\"], label=\"validation\")\n",
    "axes[2, 0].legend()\n",
    "\n",
    "axes[2, 1].set_title(\"Pixel Height Accuracy\")\n",
    "# axes[2, 1].plot(log[\"training\"][\"accuracy\"], label=\"training\")\n",
    "# axes[2, 1].plot(log[\"validation\"][\"accuracy\"], label=\"validation\")\n",
    "axes[2, 1].legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load a sample dataset for validation purposes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_folder = DATA_PATH / \"Physionet_preprocessed\"\n",
    "# train_test_folders = read_train_test_split_yaml(input_yaml=TRAIN_TEST_SPLIT_YAML, prefix_base_folder=data_folder)\n",
    "# train_folders, test_folders = train_test_folders.train, train_test_folders.test\n",
    "# del train_test_folders\n",
    "\n",
    "# Instantiate an AiDataset with exactly __one__ contained SlidingWindowDataset\n",
    "ai_dataset_config = copy.deepcopy(hyperparams[\"test_dataset_config\"])\n",
    "ai_dataset_config.dataset_folders = [DATA_PATH/\"tr12-0261\"]\n",
    "ai_dataset_config.noise_mean_std = None\n",
    "ai_dataset = AiDataset(config=ai_dataset_config)\n",
    "\n",
    "len(ai_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantitative Evaluation: Aggregate *model performance metrics* over whole ValidationDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(ai_dataset, batch_size, shuffle=False, collate_fn=TrainingBatch.from_iterable, num_workers=n_cpu_cores-1)\n",
    "\n",
    "# Iterate over the dataset and gather performance information\n",
    "overall_evaluator = ConfusionMatrixEvaluator.empty()\n",
    "for batch in tqdm(data_loader):\n",
    "    batch.to_device(model.device)\n",
    "    net_input = torch.autograd.Variable(batch.input_data)\n",
    "    net_output = model(net_input)\n",
    "    batch_evaluator = ConfusionMatrixEvaluator(model_output_batch=net_output, ground_truth_batch=batch.ground_truth)\n",
    "    overall_evaluator += batch_evaluator\n",
    "\n",
    "overall_evaluator.print_exhausting_metrics_results(include_short_summary=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apnea events vector construction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "respiratory_events = retrieve_respiratory_events(model=model, ai_dataset=ai_dataset, progress_fn=tqdm)[0]\n",
    "\n",
    "respiratory_event_type_counter = Counter([e.event_type for e in respiratory_events])\n",
    "print(\"Respiratory event types as per AI detection:\")\n",
    "print(\" - \" + \"\\n - \".join(f\"{klass.name}: {cnt}\" for klass, cnt in respiratory_event_type_counter.items()))\n",
    "print()\n",
    "print(f\"{len(respiratory_events)} detected respiratory events:\")\n",
    "print(\" - \" + \"\\n - \".join([f\"#{i}: {evt}\" for i, evt in enumerate(respiratory_events)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}